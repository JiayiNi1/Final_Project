{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import pandas as pd\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import Column, Integer, Float, String, DateTime, Date\n",
    "from sqlalchemy.schema import CreateTable\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uber Sample Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_data\n",
    "uber_sample = pd.read_csv(\"data/uber_rides_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constraint on longitude and latitude\n",
    "uber_sample = uber_sample[uber_sample['pickup_longitude']>=-74.242330]\n",
    "uber_sample = uber_sample[uber_sample['pickup_longitude']<=-73.717047]\n",
    "uber_sample = uber_sample[uber_sample['pickup_latitude']>=40.560445]\n",
    "uber_sample = uber_sample[uber_sample['pickup_latitude']<=40.908524]\n",
    "uber_sample = uber_sample[uber_sample['dropoff_longitude']>=-74.242330]\n",
    "uber_sample = uber_sample[uber_sample['dropoff_longitude']<=-73.717047]\n",
    "uber_sample = uber_sample[uber_sample['dropoff_latitude']>=40.560445]\n",
    "uber_sample = uber_sample[uber_sample['dropoff_latitude']<=40.908524]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for calculating distance given longitude and latitude of pickup and dropoff location\n",
    "def cal_distance(lat1, lat2, long1, long2):\n",
    "     \n",
    "    # convert degrees to radians\n",
    "    long1 = radians(long1)\n",
    "    long2 = radians(long2)\n",
    "    lat1 = radians(lat1)\n",
    "    lat2 = radians(lat2)\n",
    "      \n",
    "    # apply the Haversine formula\n",
    "    result = sin((lat2 - lat1) / 2)**2 + cos(lat1) * cos(lat2) * sin((long2 - long1) / 2)**2\n",
    "    result = 2 * asin(sqrt(result))*3956\n",
    "      \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test for cal_distance\n",
    "def cal_distance_test():\n",
    "    assert round(cal_distance(40, 41, 70, 71), 2) == 86.74\n",
    "cal_distance_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out unnecessary columns\n",
    "uber_cols = ['pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "uber_sample_cleaned = uber_sample[uber_cols].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate distance for each trip and add it to the dataframe\n",
    "distance = []\n",
    "for i in range(len(uber_sample_cleaned)):\n",
    "    data = uber_sample_cleaned.iloc[i,:]\n",
    "    distance.append(cal_distance(data[2], data[4], data[1], data[3]))\n",
    "uber_sample_cleaned['distance'] = distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned data to csv\n",
    "uber_sample_cleaned.to_csv(\"data/uber_sample_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count number of records in each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_sample_cleaned['year'] = uber_sample_cleaned['pickup_datetime'].apply(lambda x:int(x[:4]))\n",
    "uber_sample_cleaned['month'] = uber_sample_cleaned['pickup_datetime'].apply(lambda x:int(x[5:7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows_dict = uber_sample_cleaned.groupby(['year', 'month']).agg({'pickup_datetime':'count'}).to_dict()['pickup_datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_sample_cleaned.drop(columns=['year', 'month'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yellow Taxi Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and Sampling Yellow Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicator function to check whether the time of the trip is within our desired time range\n",
    "def isValidTime(year, month):\n",
    "    if year > 2015 or year < 2009:\n",
    "        return False\n",
    "    if year == 2015 and month > 6:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to automatically scrap all the yellow taxi data within our desired time range\n",
    "# the input parameter last_month is used to continue on unfinished downloading\n",
    "def download_yellow_taxi(startpoint=None):\n",
    "    # use requests to get the html of the yellow taxi page\n",
    "    response = requests.get(\"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\")\n",
    "\n",
    "    # use beautifulsoup to parse the html\n",
    "    soup = bs4.BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    if startpoint:\n",
    "        start = False\n",
    "    else:\n",
    "        start = True\n",
    "    # extract all the urls that point to the csv files of yellow taxi trip records\n",
    "    for row in soup.find_all(title=\"Yellow Taxi Trip Records\"):\n",
    "        url = row['href']\n",
    "        # check if the csv is within our desired time range\n",
    "        year = int(url[-11:-7])\n",
    "        month = int(url[-6:-4])\n",
    "        if (not start) and ((year, month) == startpoint):\n",
    "            start = True\n",
    "        if isValidTime(year, month) and start:\n",
    "            # download the original csv files\n",
    "            print(url)\n",
    "            data = requests.get(url)\n",
    "            fpath = \"data/yellow_taxi/yt_{}_{}.csv\".format(year, month)\n",
    "            with open(fpath, 'wb')as file:\n",
    "                file.write(data.content)\n",
    "            print(\"Downloaded Successfully\")\n",
    "        \n",
    "            temp = pd.read_csv(fpath, error_bad_lines=False)\n",
    "            temp = temp.sample(nrows_dict[(year, month)]).reset_index(drop=True)\n",
    "            \n",
    "            temp.to_csv(fpath)\n",
    "            print(\"Written Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-04.csv\n",
      "Downloaded Successfully\n",
      "Written Successfully\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-05.csv\n",
      "Downloaded Successfully\n",
      "Written Successfully\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-06.csv\n",
      "Downloaded Successfully\n",
      "Written Successfully\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-07.csv\n",
      "Downloaded Successfully\n",
      "Written Successfully\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-08.csv\n",
      "Downloaded Successfully\n",
      "Written Successfully\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-09.csv\n",
      "Downloaded Successfully\n",
      "Written Successfully\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-10.csv\n",
      "Downloaded Successfully\n",
      "Written Successfully\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-11.csv\n",
      "Downloaded Successfully\n",
      "Written Successfully\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2010-12.csv\n",
      "Downloaded Successfully\n",
      "Written Successfully\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-01.csv\n",
      "Downloaded Successfully\n",
      "Written Successfully\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-02.csv\n",
      "Downloaded Successfully\n",
      "Written Successfully\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-03.csv\n",
      "Downloaded Successfully\n",
      "Written Successfully\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-04.csv\n",
      "Downloaded Successfully\n",
      "Written Successfully\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-05.csv\n",
      "Downloaded Successfully\n",
      "Written Successfully\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-06.csv\n",
      "Downloaded Successfully\n",
      "Written Successfully\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-07.csv\n",
      "Downloaded Successfully\n",
      "Written Successfully\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-08.csv\n",
      "Downloaded Successfully\n",
      "Written Successfully\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-09.csv\n",
      "Downloaded Successfully\n",
      "Written Successfully\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-10.csv\n",
      "Downloaded Successfully\n",
      "Written Successfully\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-11.csv\n",
      "Downloaded Successfully\n",
      "Written Successfully\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-12.csv\n",
      "Downloaded Successfully\n",
      "Written Successfully\n"
     ]
    }
   ],
   "source": [
    "download_yellow_taxi((2010, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There three different column namings among all the csv\n",
    "yt_cols_1 = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'pickup_longitude', 'pickup_latitude', \n",
    "             'dropoff_longitude', 'dropoff_latitude', 'tip_amount']\n",
    "yt_cols_2 = ['Trip_Pickup_DateTime', 'Trip_Dropoff_DateTime', 'Start_Lon', 'Start_Lat',\n",
    "             'End_Lon', 'End_Lat', 'Tip_Amt']\n",
    "yt_cols_3 = ['pickup_datetime', 'dropoff_datetime', 'pickup_longitude', 'pickup_latitude',\n",
    "             'dropoff_longitude', 'dropoff_latitude', 'tip_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_yellow_taxi():\n",
    "    yt_sample = pd.DataFrame()\n",
    "    for key in nrows_dict:\n",
    "        # read in all the yellow taxi csv\n",
    "        fpath = \"data/yellow_taxi/yt_{}_{}.csv\".format(key[0], key[1])\n",
    "        temp = pd.read_csv(fpath)\n",
    "        \n",
    "        # strip the column names as some of them contain leading spaces\n",
    "        temp.columns = [i.strip() for i in temp.columns]\n",
    "        \n",
    "        # select only necessary columns\n",
    "        if 'tpep_pickup_datetime' in temp.columns:\n",
    "            temp = temp[yt_cols_1]\n",
    "        elif 'Trip_Pickup_DateTime' in temp.columns:\n",
    "            temp = temp[yt_cols_2]\n",
    "        else:\n",
    "            temp = temp[yt_cols_3]\n",
    "            \n",
    "        # set the column names to be the same\n",
    "        temp.columns = yt_cols_3\n",
    "        \n",
    "        # filter out invalid latitude and longitude\n",
    "        temp = temp[temp['pickup_longitude']>=-74.242330]\n",
    "        temp = temp[temp['pickup_longitude']<=-73.717047]\n",
    "        temp = temp[temp['pickup_latitude']>=40.560445]\n",
    "        temp = temp[temp['pickup_latitude']<=40.908524]\n",
    "        temp = temp[temp['dropoff_longitude']>=-74.242330]\n",
    "        temp = temp[temp['dropoff_longitude']<=-73.717047]\n",
    "        temp = temp[temp['dropoff_latitude']>=40.560445]\n",
    "        temp = temp[temp['dropoff_latitude']<=40.908524]\n",
    "        \n",
    "        # calculate distance\n",
    "        distance = []\n",
    "        for i in range(len(temp)):\n",
    "            data = temp.iloc[i,:]\n",
    "            distance.append(cal_distance(data[3], data[5], data[2], data[4]))\n",
    "        temp['distance'] = distance\n",
    "        \n",
    "        # combine the dataframe from each month together \n",
    "        yt_sample = pd.concat([yt_sample, temp])\n",
    "        \n",
    "    return yt_sample.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_sample_cleaned = clean_yellow_taxi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_sample_cleaned.to_csv(\"data/yt_sample_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean up precipitation record\n",
    "def transform_precipitation(p):\n",
    "    if p==0.0:\n",
    "        return p\n",
    "    elif p=='T':\n",
    "        p = 0.0\n",
    "    else:\n",
    "        if 's' in p:\n",
    "            p = p[:-1]\n",
    "        p = float(p)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check whether the date is in our desired time range\n",
    "def valid_date(date):\n",
    "    date = pd.to_datetime(date)\n",
    "    if date.month>6:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract daily weather information\n",
    "def extract_daily(year):        \n",
    "    df = pd.read_csv(\"data/{}_weather.csv\".format(year), infer_datetime_format=True)\n",
    "    daily_cols = ['DATE', 'DailyPrecipitation', 'DailyAverageWindSpeed', 'DailySustainedWindSpeed'] # necessary columns\n",
    "    df = df[daily_cols]\n",
    "    \n",
    "    # filter out data after 2015-06\n",
    "    if year == 2015:\n",
    "        df['keep'] = df['DATE'].apply(lambda x:valid_date(x))\n",
    "        df = df[df['keep']==True]\n",
    "        df.drop(columns=['keep'], inplace=True)\n",
    "\n",
    "    df.DATE = df.DATE.apply(lambda x:x[:10])\n",
    "    df = df.fillna(0.0)\n",
    "    df.DailyPrecipitation = df.DailyPrecipitation.apply(lambda x:transform_precipitation(x))\n",
    "    df = df.groupby('DATE').agg('sum').reset_index()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract hourly weather information\n",
    "def extract_hourly(year):\n",
    "    hourly_cols = ['DATE', 'HourlyPrecipitation', 'HourlyWindSpeed'] # necessary columns\n",
    "    df = pd.read_csv(\"data/{}_weather.csv\".format(year), infer_datetime_format=True)\n",
    "    df = df[hourly_cols]\n",
    "    \n",
    "    # filter out data after 2015-06\n",
    "    if year == 2015:\n",
    "        df['keep'] = df['DATE'].apply(lambda x:valid_date(x))\n",
    "        df = df[df['keep']==True]\n",
    "        df.drop(columns=['keep'], inplace=True)\n",
    "    \n",
    "    # fill in NA values with 0.0 and clean up precipitation record\n",
    "    df.fillna(0.0, inplace=True)\n",
    "    df.HourlyPrecipitation = df.HourlyPrecipitation.apply(lambda x:transform_precipitation(x))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_weather():\n",
    "    weather_hourly = pd.DataFrame() # place holder for the hourly data\n",
    "    weather_daily = pd.DataFrame() # place holder for the daily data\n",
    "    \n",
    "    # read in csv of each month and extract hourly and daily information, then combine them into the final result\n",
    "    for year in range(2009, 2016):\n",
    "        temp_hourly = extract_hourly(year)\n",
    "        temp_daily = extract_daily(year)\n",
    "        weather_hourly = pd.concat([weather_hourly, temp_hourly])\n",
    "        weather_daily = pd.concat([weather_daily, temp_daily])\n",
    "    weather_hourly.reset_index(drop=True, inplace=True)\n",
    "    weather_daily.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return weather_hourly, weather_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (9,13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (8,9,10,17) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (7,8,9,10,17,18,42,65) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (17,78) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (8,9,17,18,78) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (10,41,78) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (10,41,78) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "weather_hourly, weather_daily = clean_weather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_hourly.to_csv(\"data/weather_hourly.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_daily.to_csv(\"data/weather_daily.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to load cleaned data from previous part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_sample_cleaned = pd.read_csv(\"data/uber_sample_cleaned.csv\", index_col=0)\n",
    "yt_sample_cleaned = pd.read_csv(\"data/yt_sample_cleaned.csv\", index_col=0)\n",
    "weather_hourly = pd.read_csv(\"data/weather_hourly.csv\", index_col=0)\n",
    "weather_daily = pd.read_csv(\"data/weather_daily.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare and Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f\"sqlite:///project.db\", echo=True)\n",
    "Base = declarative_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare uber table\n",
    "class Uber(Base):\n",
    "    # YOUR CODE HERE\n",
    "    __tablename__ = \"uber\"\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    pickup_datetime = Column(DateTime)\n",
    "    pickup_longitude = Column(Float)\n",
    "    pickup_latitude = Column(Float)\n",
    "    dropoff_longitude = Column(Float)\n",
    "    dropoff_latitude = Column(Float)\n",
    "    distance = Column(Float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare yellow taxi table\n",
    "class YellowTaxi(Base):\n",
    "    # YOUR CODE HERE\n",
    "    __tablename__ = \"yellow_taxi\"\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    pickup_datetime = Column(DateTime)\n",
    "    dropoff_datetime = Column(DateTime)\n",
    "    pickup_longitude = Column(Float)\n",
    "    pickup_latitude = Column(Float)\n",
    "    dropoff_longitude = Column(Float)\n",
    "    dropoff_latitude = Column(Float)\n",
    "    tip_amount = Column(Float)\n",
    "    distance = Column(Float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare hourly weather table\n",
    "class WeatherHourly(Base):\n",
    "    __tablename__ = \"weather_hourly\"\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    DATE = Column(DateTime)\n",
    "    HourlyPrecipitation = Column(Float)\n",
    "    HourlyWindSpeed = Column(Float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare daily weather table\n",
    "class WeatherDaily(Base):\n",
    "    __tablename__ = \"weather_daily\"\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    DATE = Column(Date)\n",
    "    DailyPrecipitation = Column(Float)\n",
    "    DailyAverageWindSpeed = Column(Float)\n",
    "    DailySustainedWindSpeed = Column(Float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-21 22:54:47,829 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2022-04-21 22:54:47,830 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"uber\")\n",
      "2022-04-21 22:54:47,831 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2022-04-21 22:54:47,832 INFO sqlalchemy.engine.Engine PRAGMA temp.table_info(\"uber\")\n",
      "2022-04-21 22:54:47,833 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2022-04-21 22:54:47,835 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"yellow_taxi\")\n",
      "2022-04-21 22:54:47,835 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2022-04-21 22:54:47,836 INFO sqlalchemy.engine.Engine PRAGMA temp.table_info(\"yellow_taxi\")\n",
      "2022-04-21 22:54:47,837 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2022-04-21 22:54:47,838 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"weather_hourly\")\n",
      "2022-04-21 22:54:47,839 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2022-04-21 22:54:47,840 INFO sqlalchemy.engine.Engine PRAGMA temp.table_info(\"weather_hourly\")\n",
      "2022-04-21 22:54:47,841 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2022-04-21 22:54:47,842 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"weather_daily\")\n",
      "2022-04-21 22:54:47,842 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2022-04-21 22:54:47,844 INFO sqlalchemy.engine.Engine PRAGMA temp.table_info(\"weather_daily\")\n",
      "2022-04-21 22:54:47,844 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2022-04-21 22:54:47,846 INFO sqlalchemy.engine.Engine \n",
      "CREATE TABLE uber (\n",
      "\tid INTEGER NOT NULL, \n",
      "\tpickup_datetime DATETIME, \n",
      "\tpickup_longitude FLOAT, \n",
      "\tpickup_latitude FLOAT, \n",
      "\tdropoff_longitude FLOAT, \n",
      "\tdropoff_latitude FLOAT, \n",
      "\tdistance FLOAT, \n",
      "\tPRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "\n",
      "2022-04-21 22:54:47,846 INFO sqlalchemy.engine.Engine [no key 0.00076s] ()\n",
      "2022-04-21 22:54:47,849 INFO sqlalchemy.engine.Engine \n",
      "CREATE TABLE yellow_taxi (\n",
      "\tid INTEGER NOT NULL, \n",
      "\tpickup_datetime DATETIME, \n",
      "\tdropoff_datetime DATETIME, \n",
      "\tpickup_longitude FLOAT, \n",
      "\tpickup_latitude FLOAT, \n",
      "\tdropoff_longitude FLOAT, \n",
      "\tdropoff_latitude FLOAT, \n",
      "\ttip_amount FLOAT, \n",
      "\tdistance FLOAT, \n",
      "\tPRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "\n",
      "2022-04-21 22:54:47,851 INFO sqlalchemy.engine.Engine [no key 0.00126s] ()\n",
      "2022-04-21 22:54:47,855 INFO sqlalchemy.engine.Engine \n",
      "CREATE TABLE weather_hourly (\n",
      "\tid INTEGER NOT NULL, \n",
      "\t\"DATE\" DATETIME, \n",
      "\t\"HourlyPrecipitation\" FLOAT, \n",
      "\t\"HourlyWindSpeed\" FLOAT, \n",
      "\tPRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "\n",
      "2022-04-21 22:54:47,856 INFO sqlalchemy.engine.Engine [no key 0.00106s] ()\n",
      "2022-04-21 22:54:47,860 INFO sqlalchemy.engine.Engine \n",
      "CREATE TABLE weather_daily (\n",
      "\tid INTEGER NOT NULL, \n",
      "\t\"DATE\" DATE, \n",
      "\t\"DailyPrecipitation\" FLOAT, \n",
      "\t\"DailyAverageWindSpeed\" FLOAT, \n",
      "\t\"DailySustainedWindSpeed\" FLOAT, \n",
      "\tPRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "\n",
      "2022-04-21 22:54:47,862 INFO sqlalchemy.engine.Engine [no key 0.00165s] ()\n",
      "2022-04-21 22:54:47,864 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-21 22:54:58,931 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"uber\")\n",
      "2022-04-21 22:54:58,932 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2022-04-21 22:54:59,085 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2022-04-21 22:55:00,171 INFO sqlalchemy.engine.Engine INSERT INTO uber (id, pickup_datetime, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, distance) VALUES (?, ?, ?, ?, ?, ?, ?)\n",
      "2022-04-21 22:55:00,171 INFO sqlalchemy.engine.Engine [generated in 0.87812s] ((0, '2015-05-07 19:52:06 UTC', -73.99981689453125, 40.73835372924805, -73.99951171875, 40.72321701049805, 1.0452401206256468), (1, '2009-07-17 20:04:56 UTC', -73.994355, 40.728225, -73.99471, 40.750325, 1.5260124910108233), (2, '2009-08-24 21:45:00 UTC', -74.005043, 40.74077, -73.962565, 40.772647, 3.127281143399981), (3, '2009-06-26 08:22:21 UTC', -73.976124, 40.790844, -73.965316, 40.803349, 1.0318034471150883), (4, '2014-08-28 17:47:00 UTC', -73.925023, 40.744085, -73.97308199999998, 40.761247, 2.7789797604356656), (5, '2011-02-12 02:27:09 UTC', -73.96901899999997, 40.75591, -73.96901899999997, 40.75591, 0.0), (6, '2014-10-12 07:04:00 UTC', -73.96144699999998, 40.69396500000001, -73.871195, 40.774297, 7.28424028869458), (7, '2012-02-17 09:32:00 UTC', -73.975187, 40.745767, -74.00272, 40.743536999999996, 1.4484703740306617)  ... displaying 10 of 195472 total bound parameter sets ...  (195470, '2015-05-20 14:56:25 UTC', -73.99712371826173, 40.7254524230957, -73.98321533203125, 40.69541549682617, 2.197946056678925), (195471, '2010-05-15 04:08:00 UTC', -73.98439499999998, 40.720077, -73.985508, 40.768793, 3.364111042389129))\n",
      "2022-04-21 22:55:00,495 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "uber_sample_cleaned.to_sql('uber', con=engine, index=True, index_label='id', if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-21 22:55:25,516 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"yellow_taxi\")\n",
      "2022-04-21 22:55:25,517 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2022-04-21 22:55:25,643 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2022-04-21 22:55:26,848 INFO sqlalchemy.engine.Engine INSERT INTO yellow_taxi (id, pickup_datetime, dropoff_datetime, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, tip_amount, distance) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
      "2022-04-21 22:55:26,848 INFO sqlalchemy.engine.Engine [generated in 1.01214s] ((0, '2009-01-27 18:04:00', '2009-01-27 18:06:00', -73.982228, 40.767275, -73.975392, 40.764442, 1.3, 0.40749657390756977), (1, '2009-01-20 12:36:00', '2009-01-20 12:52:00', -73.966172, 40.753807, -74.000485, 40.737608, 0.0, 2.114858592965408), (2, '2009-01-17 02:02:05', '2009-01-17 02:10:04', -73.999893, 40.726838, -73.98330799999998, 40.722851, 0.0, 0.9104421427634034), (3, '2009-01-07 01:01:24', '2009-01-07 01:04:58', -73.97466799999998, 40.755925, -73.975961, 40.744466, 0.0, 0.7940745958021723), (4, '2009-01-10 22:10:00', '2009-01-10 22:13:39', -73.984418, 40.75617800000001, -73.97759599999998, 40.752373, 0.0, 0.4430957294281357), (5, '2009-01-20 18:17:38', '2009-01-20 18:37:23', -73.971092, 40.750869, -73.989424, 40.773298, 0.0, 1.8213518282804633), (6, '2009-01-08 17:50:00', '2009-01-08 18:16:00', -74.00415799999998, 40.7477, -73.98241199999998, 40.755758, 0.0, 1.2662039532137777), (7, '2009-01-19 16:50:00', '2009-01-19 16:58:00', -73.96844, 40.764632, -73.96235799999998, 40.776363, 0.0, 0.8701682449226986)  ... displaying 10 of 191136 total bound parameter sets ...  (191134, '2015-06-06 00:54:04', '2015-06-06 00:59:28', -73.9615936279297, 40.756172180175774, -73.96702575683594, 40.760902404785156, 1.36, 0.4328726521715464), (191135, '2015-06-02 23:35:20', '2015-06-02 23:35:27', -73.9314193725586, 40.73677444458008, -73.9304962158203, 40.73660278320313, 0.0, 0.0497296080985739))\n",
      "2022-04-21 22:55:27,256 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "yt_sample_cleaned.to_sql('yellow_taxi', con=engine, index=True, index_label='id', if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-21 22:56:06,296 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"weather_hourly\")\n",
      "2022-04-21 22:56:06,297 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2022-04-21 22:56:06,321 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2022-04-21 22:56:06,625 INFO sqlalchemy.engine.Engine INSERT INTO weather_hourly (id, \"DATE\", \"HourlyPrecipitation\", \"HourlyWindSpeed\") VALUES (?, ?, ?, ?)\n",
      "2022-04-21 22:56:06,625 INFO sqlalchemy.engine.Engine [generated in 0.24151s] ((0, '2009-01-01T00:51:00', 0.0, 18.0), (1, '2009-01-01T01:51:00', 0.0, 18.0), (2, '2009-01-01T02:51:00', 0.0, 18.0), (3, '2009-01-01T03:51:00', 0.0, 8.0), (4, '2009-01-01T04:51:00', 0.0, 11.0), (5, '2009-01-01T05:51:00', 0.0, 18.0), (6, '2009-01-01T06:51:00', 0.0, 14.0), (7, '2009-01-01T07:51:00', 0.0, 8.0)  ... displaying 10 of 72411 total bound parameter sets ...  (72409, '2015-06-30T23:59:00', 0.0, 0.0), (72410, '2015-06-30T23:59:00', 0.0, 0.0))\n",
      "2022-04-21 22:56:06,719 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "weather_hourly.to_sql('weather_hourly', con=engine, index=True, index_label='id', if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-21 22:56:16,233 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"weather_daily\")\n",
      "2022-04-21 22:56:16,234 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2022-04-21 22:56:16,239 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2022-04-21 22:56:16,253 INFO sqlalchemy.engine.Engine INSERT INTO weather_daily (id, \"DATE\", \"DailyPrecipitation\", \"DailyAverageWindSpeed\", \"DailySustainedWindSpeed\") VALUES (?, ?, ?, ?, ?)\n",
      "2022-04-21 22:56:16,255 INFO sqlalchemy.engine.Engine [generated in 0.01159s] ((0, '2009-01-01', 0.0, 0.0, 0.0), (1, '2009-01-02', 0.0, 0.0, 0.0), (2, '2009-01-03', 0.0, 0.0, 0.0), (3, '2009-01-04', 0.0, 0.0, 0.0), (4, '2009-01-05', 0.0, 0.0, 0.0), (5, '2009-01-06', 0.0, 0.0, 0.0), (6, '2009-01-07', 0.0, 0.0, 0.0), (7, '2009-01-08', 0.0, 0.0, 0.0)  ... displaying 10 of 2367 total bound parameter sets ...  (2365, '2015-06-29', 0.0, 4.8, 12.0), (2366, '2015-06-30', 0.04, 4.9, 14.0))\n",
      "2022-04-21 22:56:16,263 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "weather_daily.to_sql('weather_daily', con=engine, index=True, index_label='id', if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Schema.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE uber (\n",
      "\tid INTEGER NOT NULL, \n",
      "\tpickup_datetime DATETIME, \n",
      "\tpickup_longitude FLOAT, \n",
      "\tpickup_latitude FLOAT, \n",
      "\tdropoff_longitude FLOAT, \n",
      "\tdropoff_latitude FLOAT, \n",
      "\tdistance FLOAT, \n",
      "\tPRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(CreateTable(Uber.__table__).compile(engine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE yellow_taxi (\n",
      "\tid INTEGER NOT NULL, \n",
      "\tpickup_datetime DATETIME, \n",
      "\tdropoff_datetime DATETIME, \n",
      "\tpickup_longitude FLOAT, \n",
      "\tpickup_latitude FLOAT, \n",
      "\tdropoff_longitude FLOAT, \n",
      "\tdropoff_latitude FLOAT, \n",
      "\ttip_amount FLOAT, \n",
      "\tdistance FLOAT, \n",
      "\tPRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(CreateTable(YellowTaxi.__table__).compile(engine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE weather_hourly (\n",
      "\tid INTEGER NOT NULL, \n",
      "\t\"DATE\" DATETIME, \n",
      "\t\"HourlyPrecipitation\" FLOAT, \n",
      "\t\"HourlyWindSpeed\" FLOAT, \n",
      "\tPRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(CreateTable(WeatherHourly.__table__).compile(engine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE weather_daily (\n",
      "\tid INTEGER NOT NULL, \n",
      "\t\"DATE\" DATE, \n",
      "\t\"DailyPrecipitation\" FLOAT, \n",
      "\t\"DailyAverageWindSpeed\" FLOAT, \n",
      "\t\"DailySustainedWindSpeed\" FLOAT, \n",
      "\tPRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(CreateTable(WeatherDaily.__table__).compile(engine))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
